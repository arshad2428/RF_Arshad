{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import (\n    accuracy_score, \n    log_loss, \n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# ==================== Load Data ====================\ndf = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\ndf[\"Status_bin\"] = df[\"Status\"].map({\"D\": 0, \"C\": 1, \"CL\": 2})\n\nX = df.drop([\"Status\", \"Status_bin\", \"id\"], axis=1)\ny = df[\"Status_bin\"]\n\n# ==================== Encode categorical columns ====================\ncat_cols = X.select_dtypes(include=[\"object\"]).columns\n\n# Option 1: Use OrdinalEncoder for all categorical columns at once\n\noe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\nX[cat_cols] = oe.fit_transform(X[cat_cols].astype(str))\n\n# ==================== Fill missing values ====================\nX = X.fillna(X.median(numeric_only=True))\n\n# ==================== Cap outliers ====================\ndef cap_outliers(df, cols, lower=1, upper=99):\n    for col in cols:\n        low_val = df[col].quantile(lower / 100)\n        high_val = df[col].quantile(upper / 100)\n        df[col] = df[col].clip(low_val, high_val)\n    return df\n\nnum_cols = X.select_dtypes(include=['float64', 'int64']).columns\nX = cap_outliers(X, num_cols)\n\n# ==================== Train-test split ====================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.02, random_state=42, stratify=y\n)\n\n# ==================== Random Forest Classifier ====================\nnum_classes = len(np.unique(y_train))\n\nrf = RandomForestClassifier(\n    n_estimators=500,\n    max_depth=None,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    max_features='sqrt',\n    random_state=42,\n    n_jobs=-1\n)\n\n# Optional: Use calibrated classifier for probabilities\nmodel = CalibratedClassifierCV(base_estimator=rf, method='isotonic', cv=5)\nmodel.fit(X_train, y_train)\n\n# ==================== Evaluate on train/test ====================\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\n\n\ny_pred_prob = model.predict_proba(X_test)\nlog_l = log_loss(y_test, y_pred_prob, labels=model.classes_)\n\n\nprecision = precision_score(y_test, y_pred, average='weighted')\nrecall = recall_score(y_test, y_pred, average='weighted')\nf1 = f1_score(y_test, y_pred, average='weighted')\n\n# ======== DISPLAY ========\nprint(\"Accuracy:\", acc)\nprint(\"Log Loss:\", log_l)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# ==================== Prepare test data ====================\n# submission = pd.DataFrame({\n#     \"id\": test_ids,\n#     \"Status\": y_pred_labels\n# })\n\n# submission.to_csv(\"professor_rf_classes.csv\", index=False)\n# print(\"Submission saved as professor_rf_classes.csv\")\n# print(submission.head())\n\n\n\n\n\n\n\n# # ==================== Predict encoded classes ====================\n# y_pred_test = model.predict(x_test)\n\n# # ==================== Create new sequential ID ====================\n# new_ids = np.arange(1, len(x_test) + 1)\n\n# # ==================== Create final CSV ====================\n# submission = pd.DataFrame({\n#     \"id\": new_ids,\n#     \"Status\": y_pred_test\n# })\n\n# submission.to_csv(\"professor_rf_encoded_classes.csv\", index=False)\n# print(\"Saved as professor_rf_encoded_classes.csv\")\n# print(submission.head())\n\n\n\n\n\n#  If your dataset is SMALL (<10k rows)\n# rf = RandomForestClassifier(\n#     n_estimators=300,\n#     max_depth=None,\n#     min_samples_split=3,\n#     min_samples_leaf=1,\n#     max_features='sqrt',\n#     random_state=42,\n#     n_jobs=-1\n# )\n# ⭐ If your dataset is MEDIUM (10k–100k rows)\n# rf = RandomForestClassifier(\n#     n_estimators=600,\n#     max_depth=20,\n#     min_samples_split=5,\n#     min_samples_leaf=2,\n#     max_features='sqrt',\n#     random_state=42,\n#     n_jobs=-1\n# )\n# ⭐ If your dataset is LARGE (>100k rows)\n# rf = RandomForestClassifier(\n#     n_estimators=1200,\n#     max_depth=15,\n#     min_samples_split=10,\n#     min_samples_leaf=5,\n#     max_features='log2',\n#     random_state=42,\n#     n_jobs=-1\n# )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}